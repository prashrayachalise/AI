{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Observe the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>Reading</th>\n",
       "      <th>Writing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>81</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>73</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>89</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>83</td>\n",
       "      <td>82</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Math  Reading  Writing\n",
       "0      48       68       63\n",
       "1      62       81       72\n",
       "2      79       80       78\n",
       "3      76       83       79\n",
       "4      59       64       62\n",
       "..    ...      ...      ...\n",
       "995    72       74       70\n",
       "996    73       86       90\n",
       "997    89       87       94\n",
       "998    83       82       78\n",
       "999    66       66       72\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\student.csv\")\n",
    "df = pd.DataFrame(reader)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>Reading</th>\n",
       "      <th>Writing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>81</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Math  Reading  Writing\n",
       "0    48       68       63\n",
       "1    62       81       72\n",
       "2    79       80       78\n",
       "3    76       83       79\n",
       "4    59       64       62"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>Reading</th>\n",
       "      <th>Writing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>73</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>89</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>83</td>\n",
       "      <td>82</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Math  Reading  Writing\n",
       "995    72       74       70\n",
       "996    73       86       90\n",
       "997    89       87       94\n",
       "998    83       82       78\n",
       "999    66       66       72"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the Information of Datasets. {Hint: pd.info}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   Math     1000 non-null   int64\n",
      " 1   Reading  1000 non-null   int64\n",
      " 2   Writing  1000 non-null   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 23.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather the Descriptive info about the Dataset. {Hint: pd.describe}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>Reading</th>\n",
       "      <th>Writing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>67.290000</td>\n",
       "      <td>69.872000</td>\n",
       "      <td>68.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.085008</td>\n",
       "      <td>14.657027</td>\n",
       "      <td>15.241287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>60.750000</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>68.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>69.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>78.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>79.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Math      Reading      Writing\n",
       "count  1000.000000  1000.000000  1000.000000\n",
       "mean     67.290000    69.872000    68.616000\n",
       "std      15.085008    14.657027    15.241287\n",
       "min      13.000000    19.000000    14.000000\n",
       "25%      58.000000    60.750000    58.000000\n",
       "50%      68.000000    70.000000    69.500000\n",
       "75%      78.000000    81.000000    79.000000\n",
       "max     100.000000   100.000000   100.000000"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split your data into Feature (X) and Label (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Math</th>\n",
       "      <th>Reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Math  Reading\n",
       "0    48       68\n",
       "1    62       81\n",
       "2    79       80\n",
       "3    76       83\n",
       "4    59       64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Splitting the data into Features (X) and Labels (Y)\n",
    "X = df.iloc[:, :-1]  # Assuming all columns except the last are features\n",
    "Y = df.iloc[:, -1]   # Assuming the last column is the label\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    63\n",
       "1    72\n",
       "2    78\n",
       "3    79\n",
       "4    62\n",
       "Name: Writing, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Feature Matrix (X): (2, 1000)\n",
      "Shape of Weight Vector (W): (2, 1)\n",
      "Shape of Target Vector (Y): (1000, 1)\n",
      "Shape of Predicted Y (Y_hat): (1, 1000)\n",
      "\n",
      "Sample Feature Matrix (X):\n",
      "[[48 62 79 76 59]\n",
      " [68 81 80 83 64]]\n",
      "\n",
      "Sample Weight Vector (W):\n",
      "[[0.80987445]\n",
      " [0.25464065]]\n",
      "\n",
      "Sample Target Vector (Y):\n",
      "[[63]\n",
      " [72]\n",
      " [78]\n",
      " [79]\n",
      " [62]]\n",
      "\n",
      "Sample Predicted Y (Y_hat):\n",
      "[[56.18953792 70.83810868 84.3513336  82.68563223 64.07959421]]\n"
     ]
    }
   ],
   "source": [
    "X_matrix = X.T.to_numpy()  \n",
    "Y_matrix = Y.to_numpy().reshape(-1, 1) \n",
    "\n",
    "W = np.random.rand(X_matrix.shape[0], 1)  \n",
    "\n",
    "Y_hat = np.dot(W.T, X_matrix)  \n",
    "\n",
    "print(\"Shape of Feature Matrix (X):\", X_matrix.shape)\n",
    "print(\"Shape of Weight Vector (W):\", W.shape)\n",
    "print(\"Shape of Target Vector (Y):\", Y_matrix.shape)\n",
    "print(\"Shape of Predicted Y (Y_hat):\", Y_hat.shape)\n",
    "\n",
    "print(\"\\nSample Feature Matrix (X):\")\n",
    "print(X_matrix[:, :5]) \n",
    "\n",
    "print(\"\\nSample Weight Vector (W):\")\n",
    "print(W)\n",
    "\n",
    "print(\"\\nSample Target Vector (Y):\")\n",
    "print(Y_matrix[:5])  \n",
    "\n",
    "print(\"\\nSample Predicted Y (Y_hat):\")\n",
    "print(Y_hat[:, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (X_train) Shape: (800, 2)\n",
      "Testing Data (X_test) Shape: (200, 2)\n",
      "Training Labels (Y_train) Shape: (800,)\n",
      "Testing Labels (Y_test) Shape: (200,)\n",
      "\n",
      "First few entries of X_train:\n",
      "      Math  Reading\n",
      "29     64       82\n",
      "535    62       70\n",
      "695    36       21\n",
      "557    81       70\n",
      "836    82       86\n",
      "\n",
      "First few entries of Y_train:\n",
      " 29     78\n",
      "535    67\n",
      "695    25\n",
      "557    71\n",
      "836    87\n",
      "Name: Writing, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split the dataset into training and testing sets (80-20 split)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Transpose X back to (1000, 2)\n",
    "\n",
    "# Check the dimensions of the split data\n",
    "print(\"Training Data (X_train) Shape:\", X_train.shape)\n",
    "print(\"Testing Data (X_test) Shape:\", X_test.shape)\n",
    "print(\"Training Labels (Y_train) Shape:\", Y_train.shape)\n",
    "print(\"Testing Labels (Y_test) Shape:\", Y_test.shape)\n",
    "\n",
    "# Optionally, if you want to confirm the split visually:\n",
    "# Print the first few entries of X_train and Y_train\n",
    "print(\"\\nFirst few entries of X_train:\\n\", X_train[:5])\n",
    "print(\"\\nFirst few entries of Y_train:\\n\", Y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cost_function(X, Y, W):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix (d x n), where d is the number of features and n is the number of samples.\n",
    "    Y : numpy.ndarray\n",
    "        Target vector (n, ), where n is the number of samples.\n",
    "    W : numpy.ndarray\n",
    "        Weight vector (d, ), where d is the number of features.\n",
    "\n",
    "    Output:\n",
    "    cost : float\n",
    "        The accumulated mean squared error (MSE).\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the predicted values (Y_hat) using the linear model\n",
    "    Y_pred = np.dot(X, W)  # X * W gives the predicted values\n",
    "\n",
    "    # Step 2: Calculate the squared errors between the predicted and actual values\n",
    "    errors = Y - Y_pred\n",
    "\n",
    "    # Step 3: Compute the Mean Squared Error (MSE)\n",
    "    cost = np.mean(errors ** 2)  # MSE = average of squared errors\n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceed Further\n",
      "Cost function output: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the cost function\n",
    "def cost_function(X, Y, W):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix (d x n), where d is the number of features and n is the number of samples.\n",
    "    Y : numpy.ndarray\n",
    "        Target vector (n, ), where n is the number of samples.\n",
    "    W : numpy.ndarray\n",
    "        Weight vector (d, ), where d is the number of features.\n",
    "\n",
    "    Output:\n",
    "    cost : float\n",
    "        The accumulated mean squared error (MSE).\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the predicted values (Y_hat) using the linear model\n",
    "    Y_pred = np.dot(X, W)  # X * W gives the predicted values\n",
    "\n",
    "    # Step 2: Calculate the squared errors between the predicted and actual values\n",
    "    errors = Y - Y_pred\n",
    "\n",
    "    # Step 3: Compute the Mean Squared Error (MSE)\n",
    "    cost = np.mean(errors ** 2)  # MSE = average of squared errors\n",
    "\n",
    "    return cost\n",
    "\n",
    "# Test case\n",
    "X_test = np.array([[1, 2],\n",
    "                   [3, 4],\n",
    "                   [5, 6]])\n",
    "Y_test = np.array([3, 7, 11])\n",
    "W_test = np.array([1, 1])\n",
    "\n",
    "# Calculate the cost\n",
    "cost = cost_function(X_test, Y_test, W_test)\n",
    "\n",
    "# Check if the cost is as expected (0)\n",
    "if cost == 0:\n",
    "    print(\"Proceed Further\")\n",
    "else:\n",
    "    print(\"Something went wrong: Reimplement the cost function\")\n",
    "\n",
    "print(\"Cost function output:\", cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Weights: [ 3.34235416  1.26765577 -0.80704262]\n",
      "Final Cost: 0.009839845695018414\n",
      "Cost History (first 10 iterations): [2.1872500000000006, 1.9468728849999999, 1.9346233499605003, 1.9243856735727318, 1.9142187964431634, 1.9041057754430228, 1.894046183830464, 1.8840397381117582, 1.874086157501067, 1.8641851627063604]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the cost function\n",
    "def cost_function(X, Y, W):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix (m x n), where m is the number of samples and n is the number of features.\n",
    "    Y : numpy.ndarray\n",
    "        Target vector (m, ), where m is the number of samples.\n",
    "    W : numpy.ndarray\n",
    "        Weight vector (n, ), where n is the number of features.\n",
    "\n",
    "    Output:\n",
    "    cost : float\n",
    "        The accumulated mean squared error (MSE).\n",
    "    \"\"\"\n",
    "    # Calculate the predicted values\n",
    "    Y_pred = np.dot(X, W)\n",
    "\n",
    "    # Calculate the squared errors\n",
    "    errors = Y - Y_pred\n",
    "\n",
    "    # Compute the Mean Squared Error (MSE)\n",
    "    cost = np.mean(errors ** 2)\n",
    "\n",
    "    return cost\n",
    "\n",
    "# Define the gradient descent function\n",
    "def gradient_descent(X, Y, W, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
    "\n",
    "    Parameters:\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix (m x n).\n",
    "    Y : numpy.ndarray\n",
    "        Target vector (m x 1).\n",
    "    W : numpy.ndarray\n",
    "        Initial guess for parameters (n x 1).\n",
    "    alpha : float\n",
    "        Learning rate.\n",
    "    iterations : int\n",
    "        Number of iterations for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    W_update : numpy.ndarray\n",
    "        Updated parameters (n x 1).\n",
    "    cost_history : list\n",
    "        History of cost values over iterations.\n",
    "    \"\"\"\n",
    "    # Initialize cost history\n",
    "    cost_history = []\n",
    "\n",
    "    # Number of samples (m)\n",
    "    m = len(Y)\n",
    "\n",
    "    # Gradient descent loop\n",
    "    for iteration in range(iterations):\n",
    "        # Step 1: Hypothesis Values (hθ(X) = X * W)\n",
    "        Y_pred = np.dot(X, W)\n",
    "\n",
    "        # Step 2: Loss (Difference between predicted and actual values)\n",
    "        loss = Y_pred - Y\n",
    "\n",
    "        # Step 3: Gradient Calculation (dw = (2/m) * X^T * loss)\n",
    "        dw = (2/m) * np.dot(X.T, loss)\n",
    "\n",
    "        # Step 4: Update weights (W = W - alpha * dw)\n",
    "        W_update = W - alpha * dw\n",
    "\n",
    "        # Step 5: Calculate the new cost value and store it in cost_history\n",
    "        cost = cost_function(X, Y, W_update)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Update weights for the next iteration\n",
    "        W = W_update\n",
    "\n",
    "    return W_update, cost_history\n",
    "\n",
    "# Example Usage:\n",
    "\n",
    "# Sample dataset (Feature matrix X and target vector Y)\n",
    "X = np.array([[1, 3, 5],\n",
    "              [2, 4, 6],\n",
    "              [1, 3, 5],\n",
    "              [2, 4, 6]])  # (4 samples, 3 features)\n",
    "\n",
    "Y = np.array([3, 7, 3, 7])  # Target values (4 samples)\n",
    "\n",
    "# Initialize weights (W) randomly or set to zeros\n",
    "W_init = np.zeros(X.shape[1])  # (3 features)\n",
    "\n",
    "# Set learning rate and number of iterations\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# Call gradient descent\n",
    "W_optimal, cost_history = gradient_descent(X, Y, W_init, alpha, iterations)\n",
    "\n",
    "# Print results\n",
    "print(\"Optimized Weights:\", W_optimal)\n",
    "print(\"Final Cost:\", cost_history[-1])\n",
    "print(\"Cost History (first 10 iterations):\", cost_history[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Parameters: [0.19444407 0.46183379 0.18966481]\n",
      "Cost History: [0.2126848827535125, 0.2096956540023033, 0.20680484150778317, 0.20400913903147735, 0.2013053516475558, 0.1986903919943462, 0.1961612766520831, 0.19371512264263963, 0.19134914404713466, 0.18906064873744627, 0.186847035217795, 0.18470578957269027, 0.1826344825176583, 0.1806307665492894, 0.17869237319126174, 0.17681711033310762, 0.17500285965859919, 0.1732475741607373, 0.171549275740425, 0.16990605288600855, 0.16831605843096228, 0.16677750738708613, 0.16528867485067233, 0.16384789397918426, 0.16245355403607245, 0.16110409850143448, 0.15979802324629894, 0.15853387476839365, 0.1573102484873253, 0.15612578709717104, 0.1549791789745484, 0.15386915664029455, 0.1527944952729508, 0.15175401127230642, 0.1507465608713166, 0.14977103879476603, 0.14882637696310236, 0.1479115432399203, 0.14702554022162478, 0.14616740406785345, 0.1453362033712861, 0.14453103806551396, 0.14375103836968794, 0.14299536376870614, 0.14226320202774534, 0.14155376823997826, 0.14086630390636046, 0.14020007604640541, 0.13955437633890605, 0.13892852029159206, 0.1383218464387509, 0.13773371556586855, 0.13716350996038185, 0.13661063268766177, 0.13607450689137907, 0.13555457511743044, 0.1350502986606318, 0.13456115693341258, 0.13408664685576907, 0.13362628226576237, 0.13317959334986748, 0.13274612609250616, 0.1323254417441172, 0.1319171163071394, 0.13152074003930475, 0.1311359169736585, 0.1307622644547425, 0.13039941269039848, 0.13004700431866364, 0.1297046939892525, 0.12937214795913127, 0.12904904370171244, 0.12873506952920952, 0.1284299242277091, 0.12813331670453215, 0.12784496564747053, 0.12756459919549884, 0.12729195462057505, 0.1270267780201569, 0.1267688240200725, 0.1265178554873971, 0.12627364325299864, 0.12603596584342702, 0.12580460922183132, 0.12557936653760232, 0.12536003788444572, 0.1251464300666018, 0.12493835637293835, 0.12473563635865023, 0.12453809563431101, 0.12434556566202762, 0.12415788355845969, 0.12397489190447228, 0.12379643856119843, 0.12362237649229572, 0.12345256359218824, 0.12328686252009266, 0.12312514053963312, 0.12296726936385723, 0.12281312500547091, 0.12266258763211688, 0.12251554142652575, 0.12237187445137736, 0.1222314785187121, 0.1220942490637402, 0.12196008502289998, 0.12182888871602257, 0.12170056573246475, 0.12157502482107571, 0.1214521777838696, 0.12133193937327819, 0.12121422719286329, 0.12109896160137294, 0.12098606562002813, 0.12087546484293135, 0.12076708735049246, 0.1206608636257692, 0.12055672647362524, 0.12045461094261017, 0.1203544542494702, 0.12025619570620064, 0.12015977664955493, 0.12006514037292691, 0.11997223206052739, 0.11988099872377651, 0.11979138913983835, 0.1197033537922251, 0.11961684481340112, 0.11953181592931976, 0.1194482224058277, 0.11936602099687374, 0.11928516989446196, 0.11920562868028944, 0.11912735827901287, 0.11905032091308854, 0.11897448005913266, 0.11889980040575165, 0.11882624781279176, 0.11875378927196109, 0.11868239286877731, 0.1186120277457966, 0.1185426640670809, 0.11847427298386112, 0.11840682660135701, 0.11834029794671358, 0.11827466093801751, 0.1182098903543565, 0.11814596180688693, 0.11808285171087515, 0.11802053725868075, 0.11795899639364882, 0.11789820778488157, 0.11783815080285912, 0.11777880549588116, 0.11772015256730185, 0.11766217335353076, 0.11760484980277494, 0.11754816445449634, 0.11749210041956072, 0.1174366413610552, 0.11738177147575123, 0.1173274754761919, 0.11727373857338211, 0.1172205464600613, 0.11716788529453953, 0.1171157416850777, 0.11706410267479334, 0.1170129557270747, 0.1169622887114857, 0.11691208989014548, 0.11686234790456622, 0.11681305176293416, 0.11676419082781883, 0.11671575480429565, 0.11666773372846868, 0.11662011795637932, 0.11657289815328864, 0.1165260652833203, 0.11647961059945189, 0.11643352563384347, 0.11638780218849125, 0.11634243232619607, 0.11629740836183569, 0.11625272285393086, 0.11620836859649523, 0.1161643386111594, 0.11612062613956009, 0.11607722463598552, 0.11603412776026807, 0.11599132937091616, 0.11594882351847743, 0.1159066044391248, 0.1158646665484588, 0.11582300443551802, 0.11578161285699144, 0.1157404867316251, 0.1156996211348173, 0.11565901129339526, 0.11561865258056805, 0.11557854051104847, 0.11553867073633971, 0.11549903904018043, 0.1154596413341427, 0.11542047365337836, 0.11538153215250838, 0.11534281310165012, 0.11530431288257889, 0.11526602798501782, 0.115227955003053, 0.11519009063166885, 0.11515243166340049, 0.11511497498509783, 0.11507771757479934, 0.11504065649871023, 0.1150037889082827, 0.11496711203739393, 0.11493062319961941, 0.11489431978559729, 0.11485819926048196, 0.11482225916148278, 0.11478649709548555, 0.1147509107367542, 0.11471549782470937, 0.11468025616178183, 0.11464518361133783, 0.11461027809567442, 0.11457553759408189, 0.11454096014097125, 0.11450654382406487, 0.11447228678264759, 0.11443818720587665, 0.11440424333114853, 0.11437045344252036, 0.11433681586918443, 0.1143033289839941, 0.1142699912020388, 0.11423680097926722, 0.11420375681115641, 0.1141708572314257, 0.11413810081079387, 0.11410548615577769, 0.11407301190753111, 0.11404067674072298, 0.11400847936245288, 0.11397641851120299, 0.11394449295582504, 0.11391270149456145, 0.11388104295409919, 0.11384951618865514, 0.1138181200790926, 0.1137868535320667, 0.11375571547919903, 0.11372470487627956, 0.11369382070249555, 0.11366306195968591, 0.11363242767162093, 0.1136019168833058, 0.11357152866030752, 0.11354126208810446, 0.11351111627145734, 0.11348109033380181, 0.1134511834166606, 0.11342139467907604, 0.11339172329706079, 0.11336216846306783, 0.1133327293854773, 0.11330340528810083, 0.1132741954097026, 0.11324509900353606, 0.11321611533689614, 0.11318724369068671, 0.11315848335900207, 0.11312983364872259, 0.11310129387912381, 0.11307286338149856, 0.11304454149879156, 0.1130163275852463, 0.11298822100606365, 0.11296022113707181, 0.11293232736440723, 0.112904539084206, 0.11287685570230589, 0.11284927663395779, 0.11282180130354698, 0.1127944291443238, 0.11276715959814293, 0.11273999211521141, 0.1127129261538452, 0.11268596118023363, 0.11265909666821161, 0.11263233209903936, 0.1126056669611898, 0.11257910075014238, 0.11255263296818413, 0.1125262631242171, 0.11249999073357227, 0.11247381531782946, 0.11244773640464328, 0.11242175352757483, 0.11239586622592865, 0.1123700740445956, 0.11234437653390017, 0.11231877324945354, 0.11229326375201083, 0.11226784760733358, 0.11224252438605609, 0.1122172936635569, 0.11219215501983378, 0.11216710803938323, 0.11214215231108371, 0.11211728742808276, 0.11209251298768791, 0.11206782859126074, 0.11204323384411481, 0.1120187283554167, 0.11199431173809046, 0.11196998360872469, 0.11194574358748309, 0.1119215912980178, 0.11189752636738547, 0.11187354842596581, 0.11184965710738323, 0.11182585204843078, 0.11180213288899628, 0.11177849927199128, 0.11175495084328185, 0.11173148725162207, 0.11170810814858907, 0.11168481318852054, 0.11166160202845435, 0.11163847432806935, 0.11161542974962904, 0.11159246795792627, 0.11156958862022998, 0.11154679140623376, 0.11152407598800554, 0.11150144203993945, 0.11147888923870877, 0.11145641726322053, 0.11143402579457144, 0.11141171451600534, 0.11138948311287176, 0.11136733127258591, 0.11134525868458983, 0.11132326504031483, 0.111301350033145, 0.11127951335838189, 0.11125775471321012, 0.11123607379666436, 0.11121447030959693, 0.11119294395464666, 0.11117149443620855, 0.11115012146040444, 0.11112882473505444, 0.11110760396964939, 0.11108645887532384, 0.1110653891648303, 0.11104439455251373, 0.1110234747542871, 0.11100262948760772, 0.1109818584714539, 0.11096116142630282, 0.11094053807410843, 0.11091998813828066, 0.11089951134366456, 0.1108791074165206, 0.11085877608450506, 0.11083851707665138, 0.11081833012335189, 0.11079821495633975, 0.1107781713086721, 0.1107581989147128, 0.11073829751011642, 0.11071846683181215, 0.11069870661798854, 0.1106790166080782, 0.11065939654274354, 0.11063984616386209, 0.1106203652145131, 0.1106009534389637, 0.11058161058265586, 0.11056233639219387, 0.11054313061533153, 0.11052399300096015, 0.11050492329909685, 0.11048592126087291, 0.11046698663852256, 0.11044811918537213, 0.11042931865582933, 0.11041058480537279, 0.11039191739054202, 0.11037331616892747, 0.11035478089916084, 0.11033631134090567, 0.11031790725484827, 0.11029956840268845, 0.11028129454713102, 0.11026308545187712, 0.11024494088161578, 0.11022686060201581, 0.11020884437971787, 0.11019089198232651, 0.11017300317840259, 0.11015517773745577, 0.11013741542993732, 0.1101197160272328, 0.11010207930165511, 0.11008450502643764, 0.1100669929757276, 0.11004954292457933, 0.11003215464894792, 0.11001482792568304, 0.10999756253252232, 0.10998035824808586, 0.1099632148518698, 0.10994613212424076, 0.10992910984642996, 0.10991214780052774, 0.1098952457694779, 0.10987840353707247, 0.10986162088794614, 0.10984489760757132, 0.10982823348225286, 0.10981162829912292, 0.10979508184613632, 0.10977859391206529, 0.109762164286495, 0.10974579275981863, 0.10972947912323297, 0.10971322316873357, 0.10969702468911048, 0.10968088347794376, 0.10966479932959915, 0.10964877203922374, 0.10963280140274176, 0.10961688721685052, 0.10960102927901616, 0.10958522738746974, 0.10956948134120323, 0.10955379093996549, 0.10953815598425852, 0.1095225762753336, 0.10950705161518749, 0.10949158180655878, 0.10947616665292409, 0.10946080595849458, 0.10944549952821232, 0.10943024716774673, 0.1094150486834911, 0.10939990388255909, 0.10938481257278154, 0.1093697745627027, 0.10935478966157733, 0.10933985767936705, 0.1093249784267373, 0.10931015171505398, 0.10929537735638045, 0.10928065516347406, 0.10926598494978343, 0.1092513665294449, 0.10923679971727987, 0.1092222843287916, 0.10920782018016215, 0.10919340708824951, 0.10917904487058461, 0.10916473334536841, 0.109150472331469, 0.10913626164841882, 0.10912210111641159, 0.10910799055629983, 0.10909392978959186, 0.10907991863844897, 0.1090659569256829, 0.10905204447475286, 0.10903818110976317, 0.10902436665546018, 0.10901060093722997, 0.1089968837810955, 0.10898321501371405, 0.10896959446237471, 0.10895602195499565, 0.10894249732012176, 0.10892902038692187, 0.10891559098518647, 0.10890220894532505, 0.10888887409836379, 0.10887558627594272, 0.10886234531031383, 0.10884915103433812, 0.10883600328148352, 0.1088229018858223, 0.10880984668202878, 0.108796837505377, 0.10878387419173814, 0.10877095657757849, 0.10875808449995693, 0.10874525779652267, 0.1087324763055129, 0.10871973986575062, 0.1087070483166423, 0.10869440149817561, 0.10868179925091712, 0.10866924141601032, 0.10865672783517308, 0.10864425835069566, 0.10863183280543848, 0.10861945104282977, 0.1086071129068637, 0.10859481824209796, 0.10858256689365174, 0.10857035870720366, 0.10855819352898947, 0.10854607120580001, 0.10853399158497914, 0.10852195451442173, 0.10850995984257134, 0.10849800741841842, 0.10848609709149805, 0.10847422871188801, 0.1084624021302067, 0.1084506171976111, 0.10843887376579474, 0.1084271716869858, 0.1084155108139449, 0.10840389099996331, 0.10839231209886094, 0.10838077396498419, 0.10836927645320425, 0.10835781941891495, 0.1083464027180309, 0.1083350262069855, 0.10832368974272916, 0.10831239318272712, 0.10830113638495781, 0.10828991920791084, 0.108278741510585, 0.1082676031524866, 0.10825650399362735, 0.10824544389452267, 0.10823442271618987, 0.10822344032014602, 0.10821249656840638, 0.10820159132348253, 0.10819072444838047, 0.10817989580659867, 0.10816910526212664, 0.1081583526794427, 0.10814763792351245, 0.10813696085978691, 0.10812632135420074, 0.10811571927317043, 0.10810515448359252, 0.10809462685284205, 0.10808413624877039, 0.10807368253970397, 0.10806326559444215, 0.1080528852822558, 0.10804254147288525, 0.10803223403653893, 0.10802196284389137, 0.10801172776608164, 0.1080015286747116, 0.10799136544184426, 0.10798123794000208, 0.10797114604216523, 0.10796108962176992, 0.10795106855270702, 0.10794108270931989, 0.10793113196640312, 0.10792121619920085, 0.10791133528340496, 0.10790148909515353, 0.10789167751102932, 0.10788190040805798, 0.1078721576637065, 0.10786244915588167, 0.10785277476292837, 0.10784313436362805, 0.10783352783719719, 0.10782395506328557, 0.1078144159219748, 0.10780491029377673, 0.10779543805963192, 0.10778599910090801, 0.10777659329939819, 0.10776722053731982, 0.10775788069731247, 0.10774857366243694, 0.10773929931617328, 0.10773005754241956, 0.10772084822549008, 0.10771167125011415, 0.1077025265014344, 0.10769341386500532, 0.10768433322679183, 0.10767528447316765, 0.10766626749091399, 0.10765728216721794, 0.10764832838967103, 0.10763940604626793, 0.10763051502540465, 0.10762165521587735, 0.10761282650688092, 0.10760402878800723, 0.10759526194924408, 0.10758652588097359, 0.10757782047397062, 0.10756914561940153, 0.10756050120882295, 0.10755188713417976, 0.10754330328780438, 0.10753474956241497, 0.10752622585111409, 0.10751773204738738, 0.10750926804510214, 0.10750083373850579, 0.10749242902222493, 0.10748405379126347, 0.1074757079410016, 0.1074673913671942, 0.10745910396596967, 0.10745084563382858, 0.10744261626764211, 0.10743441576465093, 0.10742624402246377, 0.10741810093905613, 0.1074099864127689, 0.1074019003423071, 0.10739384262673841, 0.10738581316549223, 0.10737781185835786, 0.10736983860548369, 0.10736189330737549, 0.10735397586489545, 0.10734608617926066, 0.10733822415204199, 0.10733038968516263, 0.10732258268089706, 0.10731480304186956, 0.10730705067105305, 0.10729932547176783, 0.10729162734768032, 0.10728395620280176, 0.10727631194148701, 0.1072686944684334, 0.10726110368867926, 0.10725353950760294, 0.10724600183092138, 0.10723849056468904, 0.10723100561529655, 0.10722354688946975, 0.10721611429426801, 0.1072087077370835, 0.10720132712563984, 0.1071939723679907, 0.10718664337251893, 0.1071793400479351, 0.10717206230327647, 0.1071648100479058, 0.10715758319151013, 0.10715038164409962, 0.10714320531600649, 0.10713605411788356, 0.1071289279607034, 0.1071218267557572, 0.10711475041465328, 0.1071076988493163, 0.10710067197198589, 0.10709366969521565, 0.10708669193187195, 0.10707973859513281, 0.10707280959848685, 0.10706590485573203, 0.10705902428097464, 0.10705216778862812, 0.10704533529341206, 0.10703852671035101, 0.10703174195477333, 0.1070249809423102, 0.10701824358889452, 0.10701152981075976, 0.10700483952443893, 0.10699817264676342, 0.1069915290948621, 0.10698490878615997, 0.10697831163837737, 0.10697173756952878, 0.10696518649792171, 0.10695865834215582, 0.10695215302112154, 0.10694567045399962, 0.10693921056025929, 0.10693277325965794, 0.10692635847223958, 0.10691996611833407, 0.10691359611855605, 0.10690724839380387, 0.1069009228652585, 0.10689461945438275, 0.10688833808291996, 0.10688207867289316, 0.1068758411466041, 0.10686962542663211, 0.10686343143583316, 0.10685725909733886, 0.10685110833455556, 0.10684497907116317, 0.10683887123111431, 0.10683278473863332, 0.10682671951821529, 0.10682067549462497, 0.10681465259289592, 0.10680865073832951, 0.10680266985649398, 0.10679670987322332, 0.10679077071461661, 0.10678485230703676, 0.10677895457710976, 0.10677307745172365, 0.10676722085802766, 0.10676138472343105, 0.10675556897560246, 0.10674977354246877, 0.10674399835221429, 0.10673824333327975, 0.10673250841436141, 0.10672679352441021, 0.10672109859263071, 0.1067154235484802, 0.10670976832166802, 0.10670413284215423, 0.10669851704014921, 0.10669292084611236, 0.10668734419075127, 0.10668178700502107, 0.10667624922012323, 0.10667073076750491, 0.10666523157885788, 0.10665975158611783, 0.10665429072146335, 0.10664884891731513, 0.10664342610633501, 0.10663802222142518, 0.10663263719572737, 0.10662727096262181, 0.10662192345572659, 0.10661659460889661, 0.10661128435622279, 0.10660599263203138, 0.10660071937088281, 0.10659546450757108, 0.1065902279771229, 0.10658500971479676, 0.10657980965608203, 0.10657462773669849, 0.10656946389259499, 0.10656431805994908, 0.10655919017516585, 0.10655408017487733, 0.10654898799594165, 0.10654391357544198, 0.10653885685068615, 0.10653381775920547, 0.10652879623875401, 0.10652379222730804, 0.10651880566306485, 0.1065138364844423, 0.10650888463007774, 0.10650395003882739, 0.10649903264976564, 0.10649413240218393, 0.10648924923559039, 0.10648438308970867, 0.10647953390447745, 0.10647470162004953, 0.10646988617679115, 0.10646508751528108, 0.10646030557630998, 0.1064555403008796, 0.10645079163020205, 0.10644605950569895, 0.10644134386900074, 0.10643664466194604, 0.10643196182658059, 0.10642729530515696, 0.10642264504013337, 0.10641801097417314, 0.10641339305014404, 0.10640879121111739, 0.1064042054003674, 0.10639963556137041, 0.10639508163780435, 0.10639054357354771, 0.10638602131267895, 0.10638151479947588, 0.10637702397841484, 0.10637254879417, 0.10636808919161267, 0.10636364511581049, 0.10635921651202698, 0.10635480332572059, 0.10635040550254393, 0.1063460229883435, 0.10634165572915849, 0.1063373036712204, 0.10633296676095227, 0.10632864494496796, 0.10632433817007149, 0.1063200463832563, 0.10631576953170475, 0.10631150756278718, 0.10630726042406145, 0.10630302806327215, 0.10629881042834993, 0.10629460746741098, 0.10629041912875613, 0.10628624536087036, 0.10628208611242203, 0.1062779413322624, 0.10627381096942468, 0.10626969497312368, 0.10626559329275494, 0.1062615058778942, 0.10625743267829671, 0.1062533736438966, 0.10624932872480616, 0.1062452978713154, 0.10624128103389115, 0.10623727816317662, 0.10623328920999066, 0.10622931412532724, 0.1062253528603547, 0.10622140536641517, 0.106217471595024, 0.10621355149786896, 0.10620964502681, 0.10620575213387809, 0.10620187277127507, 0.10619800689137289, 0.10619415444671285, 0.10619031539000519, 0.10618648967412843, 0.10618267725212867, 0.10617887807721914, 0.10617509210277948, 0.1061713192823552, 0.106167559569657, 0.1061638129185604, 0.10616007928310488, 0.10615635861749345, 0.10615265087609198, 0.10614895601342866, 0.1061452739841935, 0.10614160474323757, 0.10613794824557249, 0.10613430444637004, 0.10613067330096125, 0.1061270547648361, 0.10612344879364272, 0.10611985534318716, 0.10611627436943244, 0.1061127058284983, 0.1061091496766604, 0.10610560587034987, 0.10610207436615282, 0.10609855512080962, 0.10609504809121453, 0.10609155323441496, 0.10608807050761104, 0.10608459986815509, 0.10608114127355096, 0.10607769468145363, 0.10607426004966854, 0.10607083733615114, 0.10606742649900625, 0.10606402749648765, 0.10606064028699752, 0.10605726482908581, 0.10605390108144976, 0.10605054900293343, 0.10604720855252711, 0.10604387968936684, 0.10604056237273385, 0.10603725656205397, 0.10603396221689733, 0.10603067929697765, 0.10602740776215175, 0.10602414757241904, 0.10602089868792115, 0.10601766106894123, 0.1060144346759035, 0.10601121946937278, 0.10600801541005403, 0.10600482245879168, 0.10600164057656933, 0.10599846972450906, 0.10599530986387115, 0.10599216095605342, 0.10598902296259072, 0.1059858958451545, 0.10598277956555244, 0.10597967408572778, 0.10597657936775884, 0.10597349537385871, 0.1059704220663745, 0.1059673594077871, 0.10596430736071072, 0.10596126588789202, 0.10595823495221018, 0.10595521451667604, 0.10595220454443177, 0.10594920499875043, 0.10594621584303536, 0.10594323704081984, 0.10594026855576667, 0.10593731035166748, 0.10593436239244255, 0.10593142464214013, 0.1059284970649361, 0.10592557962513344, 0.10592267228716185, 0.10591977501557721, 0.10591688777506131, 0.10591401053042106, 0.10591114324658837, 0.10590828588861959, 0.10590543842169498, 0.1059026008111184, 0.10589977302231683, 0.10589695502083978, 0.10589414677235906, 0.10589134824266838, 0.10588855939768249, 0.10588578020343736, 0.10588301062608925, 0.1058802506319145, 0.10587750018730911, 0.10587475925878824, 0.10587202781298576, 0.10586930581665391, 0.10586659323666286, 0.1058638900400003, 0.10586119619377087, 0.10585851166519594, 0.10585583642161313, 0.10585317043047583, 0.10585051365935279, 0.10584786607592786, 0.10584522764799933, 0.10584259834347978, 0.10583997813039545, 0.105837366976886, 0.105834764851204, 0.10583217172171454, 0.10582958755689485, 0.105827012325334, 0.10582444599573218, 0.10582188853690071, 0.10581933991776134, 0.10581680010734608, 0.10581426907479655, 0.10581174678936377, 0.10580923322040782, 0.10580672833739718, 0.10580423210990872, 0.10580174450762697, 0.10579926550034388, 0.10579679505795853, 0.1057943331504765, 0.10579187974800977, 0.1057894348207761, 0.10578699833909887, 0.10578457027340646, 0.10578215059423211, 0.10577973927221342, 0.10577733627809191, 0.10577494158271283, 0.10577255515702465, 0.10577017697207874, 0.10576780699902903, 0.10576544520913153, 0.10576309157374407, 0.10576074606432595, 0.10575840865243755, 0.10575607930973988, 0.10575375800799432, 0.10575144471906231, 0.10574913941490477, 0.10574684206758202, 0.10574455264925325, 0.10574227113217625, 0.10573999748870692, 0.10573773169129916, 0.10573547371250429, 0.10573322352497082, 0.10573098110144401, 0.10572874641476572, 0.10572651943787385, 0.10572430014380199, 0.10572208850567938]\n"
     ]
    }
   ],
   "source": [
    "# Generate random test data\n",
    "np.random.seed(0) # For reproducibility\n",
    "X = np.random.rand(100, 3) # 100 samples, 3 features\n",
    "Y = np.random.rand(100)\n",
    "W = np.random.rand(3) # Initial guess for parameters\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "# Test the gradient_descent function\n",
    "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
    "# Print the final parameters and cost history\n",
    "print(\"Final Parameters:\", final_params)\n",
    "print(\"Cost History:\", cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Model Evaluation - RMSE\n",
    "def rmse(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    This function calculates the Root Mean Squared Error (RMSE).\n",
    "\n",
    "    Parameters:\n",
    "    Y : numpy.ndarray\n",
    "        Array of actual (target) dependent variables (m, ).\n",
    "    Y_pred : numpy.ndarray\n",
    "        Array of predicted dependent variables (m, ).\n",
    "\n",
    "    Returns:\n",
    "    rmse : float\n",
    "        The root mean squared error.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the squared differences between actual and predicted values\n",
    "    squared_differences = (Y - Y_pred) ** 2\n",
    "\n",
    "    # Step 2: Calculate the mean of the squared differences\n",
    "    mean_squared_error = np.mean(squared_differences)\n",
    "\n",
    "    # Step 3: Take the square root of the mean squared error\n",
    "    rmse_value = np.sqrt(mean_squared_error)\n",
    "\n",
    "    return rmse_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Evaluation - R-squared\n",
    "def r2(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    This function calculates the R Squared value, which measures the goodness of fit.\n",
    "\n",
    "    Parameters:\n",
    "    Y : numpy.ndarray\n",
    "        Array of actual (target) dependent variables (m, ).\n",
    "    Y_pred : numpy.ndarray\n",
    "        Array of predicted dependent variables (m, ).\n",
    "\n",
    "    Returns:\n",
    "    r2 : float\n",
    "        The R-squared value.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate the mean of the actual values (Y)\n",
    "    mean_y = np.mean(Y)\n",
    "\n",
    "    # Step 2: Calculate the Total Sum of Squares (SST)\n",
    "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
    "\n",
    "    # Step 3: Calculate the Sum of Squared Residuals (SSR)\n",
    "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
    "\n",
    "    # Step 4: Calculate the R-squared value\n",
    "    r2_value = 1 - (ss_res / ss_tot)\n",
    "\n",
    "    return r2_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Weights: [0.34811659 0.64614558]\n",
      "Cost History (First 10 iterations): [2013.165570783755, 1640.2868325996924, 1337.061999490158, 1090.4794892850578, 889.9583270083234, 726.8940993009545, 594.2897260808594, 486.4552052951634, 398.7634463599482, 327.45171473246876]\n",
      "RMSE on Test Set: 5.2798239764188635\n",
      "R-Squared on Test Set: 0.8886354462786421\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Gradient Descent Function (as you wrote earlier)\n",
    "def gradient_descent(X, Y, W, alpha, iterations):\n",
    "    cost_history = [0] * iterations\n",
    "    m = len(Y)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # Step 1: Hypothesis Values\n",
    "        Y_pred = np.dot(X, W)  # Predicted Y values\n",
    "        # Step 2: Difference between Hypothesis and Actual Y\n",
    "        loss = Y_pred - Y\n",
    "        # Step 3: Gradient Calculation\n",
    "        dw = (1/m) * np.dot(X.T, loss)  # Gradient for the weights\n",
    "        # Step 4: Updating Values of W using Gradient\n",
    "        W -= alpha * dw\n",
    "        # Step 5: New Cost Value\n",
    "        cost = cost_function(X, Y, W)\n",
    "        cost_history[iteration] = cost\n",
    "\n",
    "    return W, cost_history\n",
    "\n",
    "# Cost Function\n",
    "def cost_function(X, Y, W):\n",
    "    m = len(Y)\n",
    "    Y_pred = np.dot(X, W)\n",
    "    cost = (1 / (2 * m)) * np.sum(np.square(Y_pred - Y))\n",
    "    return cost\n",
    "\n",
    "# Model Evaluation - RMSE\n",
    "def rmse(Y, Y_pred):\n",
    "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
    "\n",
    "# Model Evaluation - R2\n",
    "def r2(Y, Y_pred):\n",
    "    ss_tot = np.sum((Y - np.mean(Y)) ** 2)\n",
    "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    # Step 1: Load the dataset\n",
    "    data = pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\student.csv\")\n",
    "\n",
    "    # Step 2: Split the data into features (X) and target (Y)\n",
    "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
    "    Y = data['Writing'].values  # Target: Writing marks\n",
    "\n",
    "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
    "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
    "    alpha = 0.00001  # Learning rate\n",
    "    iterations = 1000  # Number of iterations for gradient descent\n",
    "\n",
    "    # Step 5: Perform Gradient Descent\n",
    "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
    "\n",
    "    # Step 6: Make predictions on the test set\n",
    "    Y_pred = np.dot(X_test, W_optimal)\n",
    "\n",
    "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
    "    model_rmse = rmse(Y_test, Y_pred)\n",
    "    model_r2 = r2(Y_test, Y_pred)\n",
    "\n",
    "    # Step 8: Output the results\n",
    "    print(\"Final Weights:\", W_optimal)\n",
    "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
    "    print(\"RMSE on Test Set:\", model_rmse)\n",
    "    print(\"R-Squared on Test Set:\", model_r2)\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimenting with Learning Rate: 0.0001\n",
      "Final Weights: [0.0894932  0.89504864]\n",
      "Final Cost (Last Iteration): 10.26076310841341\n",
      "RMSE on Test Set: 4.792607360540954\n",
      "R-Squared on Test Set: 0.908240340333986\n",
      "--------------------------------------------------\n",
      "Experimenting with Learning Rate: 0.001\n",
      "Final Weights: [nan nan]\n",
      "Final Cost (Last Iteration): nan\n",
      "RMSE on Test Set: nan\n",
      "R-Squared on Test Set: nan\n",
      "--------------------------------------------------\n",
      "Experimenting with Learning Rate: 0.01\n",
      "Final Weights: [nan nan]\n",
      "Final Cost (Last Iteration): nan\n",
      "RMSE on Test Set: nan\n",
      "R-Squared on Test Set: nan\n",
      "--------------------------------------------------\n",
      "Experimenting with Learning Rate: 0.1\n",
      "Final Weights: [nan nan]\n",
      "Final Cost (Last Iteration): nan\n",
      "RMSE on Test Set: nan\n",
      "R-Squared on Test Set: nan\n",
      "--------------------------------------------------\n",
      "Experimenting with Learning Rate: 0.5\n",
      "Final Weights: [nan nan]\n",
      "Final Cost (Last Iteration): nan\n",
      "RMSE on Test Set: nan\n",
      "R-Squared on Test Set: nan\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11096\\2158548834.py:9: RuntimeWarning: overflow encountered in square\n",
      "  cost = (1/(2*m)) * np.sum((Y_pred - Y) ** 2)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11096\\2158548834.py:20: RuntimeWarning: invalid value encountered in subtract\n",
      "  W = W - alpha * dw\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to calculate Cost (Mean Squared Error)\n",
    "def cost_function(X, Y, W):\n",
    "    m = len(Y)\n",
    "    Y_pred = np.dot(X, W)\n",
    "    cost = (1/(2*m)) * np.sum((Y_pred - Y) ** 2)\n",
    "    return cost\n",
    "\n",
    "# Gradient Descent function\n",
    "def gradient_descent(X, Y, W, alpha, iterations):\n",
    "    cost_history = []\n",
    "    m = len(Y)\n",
    "    for _ in range(iterations):\n",
    "        Y_pred = np.dot(X, W)\n",
    "        loss = Y_pred - Y\n",
    "        dw = (1/m) * np.dot(X.T, loss)\n",
    "        W = W - alpha * dw\n",
    "        cost = cost_function(X, Y, W)\n",
    "        cost_history.append(cost)\n",
    "    return W, cost_history\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "def rmse(Y, Y_pred):\n",
    "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
    "\n",
    "# R-Squared function\n",
    "def r2(Y, Y_pred):\n",
    "    ss_tot = np.sum((Y - np.mean(Y)) ** 2)\n",
    "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Main Function to integrate all steps\n",
    "def main():\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\student.csv\")  # Replace with your actual CSV file\n",
    "    X = data[['Math', 'Reading']].values  # Features\n",
    "    Y = data['Writing'].values  # Target\n",
    "\n",
    "    # Split the dataset into training and test sets (80% train, 20% test)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize weights, number of iterations, and learning rates\n",
    "    W_initial = np.zeros(X_train.shape[1])\n",
    "    iterations = 1000\n",
    "\n",
    "    # Experiment with different learning rates\n",
    "    learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "\n",
    "    for alpha in learning_rates:\n",
    "        print(f\"Experimenting with Learning Rate: {alpha}\")\n",
    "\n",
    "        # Train the model using Gradient Descent\n",
    "        W_optimal, cost_history = gradient_descent(X_train, Y_train, W_initial, alpha, iterations)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        Y_pred = np.dot(X_test, W_optimal)\n",
    "\n",
    "        # Evaluate the model\n",
    "        model_rmse = rmse(Y_test, Y_pred)\n",
    "        model_r2 = r2(Y_test, Y_pred)\n",
    "\n",
    "        # Output the results\n",
    "        print(\"Final Weights:\", W_optimal)\n",
    "        print(\"Final Cost (Last Iteration):\", cost_history[-1])\n",
    "        print(\"RMSE on Test Set:\", model_rmse)\n",
    "        print(\"R-Squared on Test Set:\", model_r2)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
